# üñ•Ô∏è **Ecliptica Smart Contract Execution Node Hardware Requirements**

**Version:** 0.1 (Draft)  
**Date:** October 2025  
**Document ID:** `ECLIPT-SC-HARDWARE-001`  
**Depends On:** `ECLIPT-CONTRACT-001`, `ECLIPT-STATE-001`

---

## 1. Executive Summary

This document specifies hardware requirements for nodes executing Ecliptica smart contracts. Given the performance targets of **10,000 TPS for contract execution** and the computational overhead of privacy-preserving operations (30-50% gas overhead), nodes require significantly more resources than standard blockchain validators.

**Key Requirements:**
- Support for high-throughput WASM execution (Wasmtime with JIT)
- Post-quantum cryptographic operations (ML-KEM, ML-DSA)
- Large memory footprint for contract state caching
- Fast NVMe storage for encrypted state access (<1ms per key)
- High network bandwidth for contract deployment and inter-contract calls

---

## 2. Node Tiers

### 2.1 Node Classification

```rust
enum ContractNodeTier {
    // Minimal viable execution node
    Basic {
        target_tps: 1000,
        max_contracts: 100,
        serves_rpc: false,
    },
    
    // Production validator with contract execution
    Standard {
        target_tps: 5000,
        max_contracts: 1000,
        serves_rpc: true,
    },
    
    // High-performance contract execution
    Performance {
        target_tps: 10000,
        max_contracts: 10000,
        serves_rpc: true,
    },
    
    // Archive node with full contract history
    Archive {
        target_tps: 10000,
        max_contracts: 100000,
        serves_rpc: true,
        full_history: true,
    },
}
```

---

## 3. Hardware Specifications by Tier

### 3.1 Basic Tier (Minimum Viable)

**Target:** Small validators, development nodes

| Component   | Specification                    | Rationale                                        |
| ----------- | -------------------------------- | ------------------------------------------------ |
| **CPU**     | 8 cores / 16 threads @ 3.5+ GHz  | WASM JIT compilation, crypto operations          |
|             | Modern x86_64 (2020+)            | AVX2 for ML-KEM/ML-DSA acceleration              |
|             | Intel: i7-11700K or equivalent   | -                                                |
|             | AMD: Ryzen 7 5800X or equivalent | -                                                |
| **RAM**     | 32 GB DDR4-3200                  | Contract instance pooling (1000 instances)       |
|             | ECC recommended                  | Data integrity for consensus-critical operations |
| **Storage** | 1 TB NVMe SSD                    | Encrypted contract state, code storage           |
|             | Read: 3000+ MB/s                 | <1ms storage access requirement                  |
|             | Write: 2000+ MB/s                | State commitment writes                          |
|             | IOPS: 100k+ random read          | Key-value lookups                                |
| **Network** | 500 Mbps symmetric               | Contract deployment, cross-contract calls        |
|             | <50ms latency to peers           | Real-time execution coordination                 |
|             | Unmetered bandwidth              | No data caps                                     |

**Cost Estimate:** $1,500-2,000 (hardware only)

---

### 3.2 Standard Tier (Production Validator)

**Target:** Mainnet validators, RPC providers

| Component   | Specification                    | Rationale                                    |
| ----------- | -------------------------------- | -------------------------------------------- |
| **CPU**     | 16 cores / 32 threads @ 4.0+ GHz | 5000 TPS target, parallel contract execution |
|             | Intel: Xeon E-2388G / i9-12900K  | -                                            |
|             | AMD: Ryzen 9 5950X / EPYC 7313P  | -                                            |
|             | AVX-512 support preferred        | 2x speedup for PQ crypto                     |
| **RAM**     | 64 GB DDR4-3600+                 | 5000 concurrent contract instances           |
|             | ECC required                     | Validator-grade reliability                  |
| **Storage** | 2 TB NVMe SSD (Gen4)             | Growing state size, contract history         |
|             | Read: 5000+ MB/s                 | High-throughput state access                 |
|             | Write: 4000+ MB/s                | Burst writes during block finalization       |
|             | IOPS: 500k+ random read          | Multiple contracts accessing storage         |
|             | Optional: 4 TB HDD for backups   | State snapshots                              |
| **Network** | 1 Gbps symmetric                 | Contract deployment (1 MB WASM), RPC traffic |
|             | <30ms to validator peers         | Consensus participation                      |
|             | 10 TB/month bandwidth            | RPC queries, state sync                      |
| **Power**   | 300-400W under load              | Energy considerations                        |
|             | UPS recommended                  | Prevent state corruption                     |

**Cost Estimate:** $3,000-4,500 (hardware only)

---

### 3.3 Performance Tier (High-Throughput)

**Target:** Large validators, enterprise RPC nodes

| Component               | Specification                    | Rationale                             |
| ----------------------- | -------------------------------- | ------------------------------------- |
| **CPU**                 | 32 cores / 64 threads @ 4.5+ GHz | 10,000 TPS target                     |
|                         | Intel: Xeon W-3375 / i9-13900KS  | -                                     |
|                         | AMD: Ryzen 9 7950X / EPYC 7443P  | -                                     |
|                         | AVX-512 + AES-NI                 | Hardware crypto acceleration          |
| **RAM**                 | 128 GB DDR5-4800+                | 10,000 concurrent instances + caching |
|                         | ECC required                     | Mission-critical reliability          |
| **Storage (Primary)**   | 4 TB NVMe SSD (Gen4)             | Active contract state                 |
|                         | Samsung 990 PRO or equivalent    | Proven endurance                      |
|                         | Read: 7000+ MB/s                 | Extreme throughput                    |
|                         | Write: 5000+ MB/s                | -                                     |
|                         | IOPS: 1M+ random read            | Max performance                       |
| **Storage (Secondary)** | 8 TB NVMe SSD (Gen3)             | Contract code, historical state       |
| **Network**             | 10 Gbps symmetric                | Large-scale RPC, state sync           |
|                         | <20ms to validator peers         | Low-latency consensus                 |
|                         | 50 TB/month bandwidth            | Heavy RPC load                        |
| **Network Card**        | Intel X550 / Mellanox ConnectX   | Offload TCP/IP processing             |
| **Power**               | 600-800W under load              | High-performance components           |

**Cost Estimate:** $8,000-12,000 (hardware only)

---

### 3.4 Archive Tier (Full History)

**Target:** Block explorers, data analytics, compliance

| Component          | Specification                    | Rationale                              |
| ------------------ | -------------------------------- | -------------------------------------- |
| **CPU**            | 32 cores / 64 threads @ 4.5+ GHz | Historical queries + current execution |
|                    | Same as Performance Tier         | -                                      |
| **RAM**            | 256 GB DDR5-4800+                | Large contract state cache             |
|                    | ECC required                     | Long-running stability                 |
| **Storage (Hot)**  | 8 TB NVMe SSD (Gen4) RAID 1      | Recent contract state (6 months)       |
|                    | 2x 8 TB in mirror                | Redundancy for critical data           |
| **Storage (Warm)** | 32 TB NVMe SSD (Gen3) RAID 5     | Historical state (2+ years)            |
|                    | 4x 8 TB with parity              | Balance capacity/performance           |
| **Storage (Cold)** | 100 TB HDD (7200 RPM) RAID 6     | Complete archival history              |
|                    | 6x 20 TB with dual parity        | Long-term retention                    |
| **Network**        | 10 Gbps symmetric                | State sync, large historical queries   |
|                    | 100 TB/month bandwidth           | Archive data distribution              |
| **Backup**         | Offsite replication              | Disaster recovery                      |

**Cost Estimate:** $15,000-25,000 (hardware only)

---

## 4. Detailed Component Requirements

### 4.1 CPU Requirements

**Instruction Sets Required:**
- **x86-64-v3** (minimum): AVX2, BMI1, BMI2, F16C, FMA, LZCNT, MOVBE
- **x86-64-v4** (recommended): AVX-512F, AVX-512BW, AVX-512CD, AVX-512DQ, AVX-512VL

**Why These Matter:**
```rust
// ML-KEM encryption with AVX2 vs AVX-512
fn benchmark_ml_kem_encapsulation() {
    // AVX2: 1.2ms per operation
    // AVX-512: 0.6ms per operation (2x faster)
    // At 10,000 TPS with 50% using crypto: 5000 ops/s
    // AVX-512 saves: 3 seconds of CPU time per second of wall time
}
```

**Thermal Design Power (TDP):**
- Basic: 65-95W
- Standard: 125-150W
- Performance: 150-250W
- Archive: 150-250W

**Core Allocation Strategy:**
```rust
// Recommended CPU pinning for 32-core system
const WASMTIME_WORKERS: usize = 20;    // Contract execution
const CRYPTO_WORKERS: usize = 6;       // ML-KEM/ML-DSA operations
const NETWORK_WORKERS: usize = 2;      // P2P message handling
const STORAGE_WORKERS: usize = 2;      // RocksDB background compaction
const SYSTEM_RESERVED: usize = 2;      // OS, monitoring

// NUMA-aware: Keep contracts + their storage on same NUMA node
```

---

### 4.2 Memory Requirements

**Memory Layout for Standard Tier (64 GB):**
```rust
struct MemoryAllocation {
    wasmtime_instances: 24_GB,   // 1000 contracts @ 24 MB each
    contract_code_cache: 8_GB,   // Compiled WASM modules
    rocksdb_cache: 16_GB,        // LRU cache for encrypted state
    os_and_networking: 8_GB,     // Kernel, TCP buffers, P2P
    consensus_state: 4_GB,       // Validator state, mempool
    monitoring: 2_GB,            // Prometheus, logs
    reserved: 2_GB,              // Headroom for spikes
}
```

**Why ECC Matters:**
```rust
// Scenario: Bit flip in encrypted contract state
// Without ECC: Silent data corruption ‚Üí wrong state root ‚Üí consensus failure
// With ECC: Bit flip detected and corrected ‚Üí no impact
// 
// For validators, ECC is *required* to maintain consensus.
```

**Memory Speed Impact:**
```rust
// Contract storage access pattern (random reads)
// DDR4-2400: ~30 GB/s random read
// DDR4-3200: ~40 GB/s random read (+33%)
// DDR5-4800: ~60 GB/s random read (+100% vs DDR4-2400)
//
// At 10,000 storage reads/sec: DDR5 provides noticeable improvement
```

---

### 4.3 Storage Requirements

**Storage Hierarchy:**

```rust
struct StorageLayout {
    // Tier 1: Hot data (NVMe Gen4)
    active_contract_code: 50_GB,      // ~50k deployed contracts @ 1 MB each
    active_state: 500_GB,             // Encrypted KV store for active contracts
    mempool: 10_GB,                   // Pending transactions
    consensus_db: 20_GB,              // Validator state, checkpoints
    
    // Tier 2: Warm data (NVMe Gen3)
    historical_state: 1_TB,           // State snapshots (7 days)
    contract_events: 200_GB,          // Encrypted event logs
    
    // Tier 3: Cold data (HDD, Archive tier only)
    full_history: 20_TB,              // Complete state history
    pruned_snapshots: 10_TB,          // Monthly snapshots
    
    // Growth projection
    daily_growth: 50_GB,              // At 10,000 TPS average
    yearly_growth: 18_TB,             // Aggressive estimate
}
```

**IOPS Requirements:**
```rust
// Worst-case scenario: 10,000 TPS, 50% using storage
fn calculate_storage_iops() -> u64 {
    let storage_txs_per_sec = 5000;
    let reads_per_tx = 10;         // Average key lookups
    let writes_per_tx = 5;         // Average key updates
    
    let read_iops = storage_txs_per_sec * reads_per_tx;   // 50k IOPS
    let write_iops = storage_txs_per_sec * writes_per_tx; // 25k IOPS
    
    // With safety margin: 100k+ random read IOPS required
    100_000
}
```

**Why NVMe Over SATA:**
```
SATA SSD:  ~95k IOPS random 4k read  ‚Üí Bottleneck at 5k TPS
NVMe Gen3: ~500k IOPS random 4k read ‚Üí Good for 10k TPS
NVMe Gen4: ~1M IOPS random 4k read   ‚Üí Headroom for spikes

Latency comparison:
SATA SSD:  ~100¬µs
NVMe Gen3: ~20¬µs  (5x faster)
NVMe Gen4: ~10¬µs  (10x faster)

For <1ms storage access requirement: NVMe is mandatory.
```

**Endurance Considerations:**
```rust
// TBW (Terabytes Written) calculation for 3-year lifespan
fn calculate_required_tbw(tier: NodeTier) -> u64 {
    let daily_writes_gb = match tier {
        Basic => 100,        // 100 GB/day
        Standard => 300,     // 300 GB/day
        Performance => 600,  // 600 GB/day
        Archive => 1000,     // 1 TB/day
    };
    
    let days_in_3_years = 1095;
    let tbw_required = (daily_writes_gb * days_in_3_years) / 1024;
    
    // Add 50% safety margin
    tbw_required * 3 / 2
}

// Result:
// Basic: ~160 TBW       ‚Üí Consumer NVMe (Samsung 980 PRO: 600 TBW)
// Standard: ~480 TBW    ‚Üí Prosumer (Samsung 990 PRO: 1200 TBW)
// Performance: ~960 TBW ‚Üí Enterprise (Intel P5800X: 27 PBW)
// Archive: ~1600 TBW    ‚Üí Enterprise with RAID
```

---

### 4.4 Network Requirements

**Bandwidth Breakdown (Standard Tier):**
```rust
struct NetworkUsage {
    // Inbound
    transactions_per_block: 50_KB,    // ~500 txs @ 100 bytes each
    blocks_per_day: 14_400,           // 6s block time
    daily_block_data: 720_MB,
    
    cross_shard_messages: 2_GB_per_day,
    contract_deployments: 500_MB_per_day,  // ~500 contracts @ 1 MB
    state_sync_p2p: 1_GB_per_day,
    
    // Outbound (if serving RPC)
    rpc_query_responses: 5_GB_per_day,
    state_proofs: 1_GB_per_day,
    
    // Total
    daily_total: 10_GB,
    monthly_total: 300_GB,
    
    // Peak (during state sync or large contract deployment)
    peak_inbound: 100_Mbps,
    peak_outbound: 50_Mbps,
}
```

**Why 1 Gbps Symmetric:**
```rust
// Large contract deployment scenario
fn deploy_large_contract() {
    let wasm_size = 1_MB;  // Maximum allowed
    let deployment_time_target = 100_ms;  // From spec
    
    // Required bandwidth
    let bits = wasm_size * 8;
    let required_mbps = bits / deployment_time_target;
    // = 8 Mbps for the contract itself
    
    // But also need to:
    // - Broadcast to 100 peers (gossip)
    // - Handle concurrent deployments
    // - Serve RPC queries
    // Total: 1 Gbps provides comfortable headroom
}
```

**Latency Requirements:**
```rust
// Cross-contract call latency budget: <5ms (from spec)
fn latency_breakdown() {
    let network_roundtrip = 1_ms;      // <30ms to peers, use 1ms for local
    let encryption = 1_ms;             // ML-KEM encrypt call
    let execution = 2_ms;              // WASM execution
    let storage_access = 1_ms;         // State read/write
    
    let total = 5_ms;
    
    // Network latency >30ms ‚Üí Fails the 5ms cross-contract call target
}
```

---

## 5. Specialized Hardware Considerations

### 5.1 Hardware Crypto Acceleration

**Intel QuickAssist Technology (QAT):**
```rust
// QAT can offload:
// - AES-GCM encryption (though ML-KEM doesn't use this)
// - SHA-256 hashing (for Merkle trees)
// - Public key operations (but not PQ algorithms yet)
//
// Current verdict: NOT recommended
// - PQ crypto (ML-KEM, ML-DSA) not supported
// - Software implementations with AVX-512 are faster for our use case
```

**GPU Acceleration:**
```rust
// Potential use cases:
// - ZK-STARK proof generation (parallel field arithmetic)
// - Batch ML-DSA signature verification
//
// Current verdict: Optional for Archive tier
// - Main bottleneck is storage, not crypto
// - GPU adds cost/complexity
// - Future: GPU-accelerated ZK proofs may become critical
```

### 5.2 NVMe Optimization

**Queue Depth:**
```rust
// NVMe queue depth for random 4k reads
const OPTIMAL_QUEUE_DEPTH: u32 = 128;

// RocksDB tuning
fn configure_rocksdb() -> Options {
    let mut opts = Options::default();
    opts.set_max_background_jobs(4);
    opts.set_max_subcompactions(2);
    opts.set_bytes_per_sync(1048576);  // 1 MB
    
    // Direct I/O to bypass page cache for large sequential scans
    opts.set_use_direct_reads(true);
    opts.set_use_direct_io_for_flush_and_compaction(true);
    
    opts
}
```

**Multi-Drive Configuration:**
```rust
// Performance tier: 2 NVMe drives
fn configure_storage_split() {
    // Drive 1: Active state (hot path)
    let drive1_mount = "/mnt/nvme0";
    let hot_db_path = format!("{}/state", drive1_mount);
    
    // Drive 2: Contract code, history (warm path)
    let drive2_mount = "/mnt/nvme1";
    let code_db_path = format!("{}/code", drive2_mount);
    let history_db_path = format!("{}/history", drive2_mount);
    
    // Reduces I/O contention between code fetching and state updates
}
```

---

## 6. Operating System & Software Requirements

### 6.1 Operating System

**Recommended:**
- **Ubuntu Server 22.04 LTS** (most tested)
- **Debian 12** (stable alternative)
- **RHEL 9** (enterprise)

**Kernel Requirements:**
- Linux kernel 5.15+ (for io_uring support)
- Kernel 6.1+ recommended (performance improvements)

**Required Kernel Modules:**
```bash
# Check support
lsmod | grep -E "nvme|aesni|avx"

# Required modules
modprobe nvme          # NVMe driver
modprobe aesni_intel   # AES-NI acceleration
modprobe sha256_ssse3  # SHA-256 acceleration
```

### 6.2 System Tuning

**Network Stack:**
```bash
# /etc/sysctl.conf
net.core.rmem_max = 134217728          # 128 MB receive buffer
net.core.wmem_max = 134217728          # 128 MB send buffer
net.ipv4.tcp_rmem = 4096 87380 67108864
net.ipv4.tcp_wmem = 4096 65536 67108864
net.core.netdev_max_backlog = 5000
net.ipv4.tcp_congestion_control = bbr  # BBR congestion control
```

**File Descriptors:**
```bash
# /etc/security/limits.conf
ecliptica soft nofile 1048576
ecliptica hard nofile 1048576

# Verify
ulimit -n
```

**CPU Governor:**
```bash
# Use performance governor for validators
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
```

**Huge Pages (2 MB pages for RocksDB):**
```bash
# /etc/sysctl.conf
vm.nr_hugepages = 8192  # 16 GB of huge pages

# Verify
cat /proc/meminfo | grep Huge
```

---

## 7. Monitoring & Benchmarking

### 7.1 Hardware Validation

**CPU Benchmark:**
```bash
# Install sysbench
apt install sysbench

# CPU test (should complete in <10s for Performance tier)
sysbench cpu --cpu-max-prime=20000 --threads=32 run
```

**Memory Benchmark:**
```bash
# Memory bandwidth test
sysbench memory --memory-block-size=1M --memory-total-size=100G run

# Target: >40 GB/s for DDR4, >60 GB/s for DDR5
```

**Storage Benchmark:**
```bash
# Install fio
apt install fio

# Random 4k read test (matches contract storage access pattern)
fio --name=random-read \
    --ioengine=libaio \
    --rw=randread \
    --bs=4k \
    --numjobs=8 \
    --size=10G \
    --runtime=60 \
    --time_based \
    --direct=1 \
    --directory=/mnt/nvme0

# Target IOPS:
# Basic: 100k+
# Standard: 300k+
# Performance: 500k+
```

**Network Benchmark:**
```bash
# Install iperf3
apt install iperf3

# Bandwidth test to peer
iperf3 -c <peer-ip> -t 60 -P 4

# Target: 900+ Mbps for 1 Gbps link
```

### 7.2 Runtime Monitoring

**Key Metrics:**
```rust
struct NodeHealthMetrics {
    // CPU
    cpu_usage_percent: f64,          // Target: <80% average
    cpu_steal_time: f64,             // Target: <1% (cloud VMs)
    
    // Memory
    memory_used_gb: u64,             // Monitor for leaks
    swap_used_gb: u64,               // Target: 0 (should never swap)
    
    // Storage
    disk_read_iops: u64,             // Compare to benchmark
    disk_write_iops: u64,
    disk_latency_avg_us: u64,        // Target: <500¬µs avg, <5ms p99
    disk_used_percent: f64,          // Alert at 80%
    
    // Network
    network_in_mbps: f64,
    network_out_mbps: f64,
    network_errors_per_sec: u64,     // Target: 0
    
    // Contract execution
    contracts_per_sec: u64,          // Target: 1000-10000 depending on tier
    avg_execution_time_ms: f64,      // Target: <10ms
    gas_used_per_sec: u64,
    
    // Alerts
    alerts: Vec<Alert>,
}

enum Alert {
    HighCPU { threshold: f64 },
    HighMemory { threshold: u64 },
    DiskFull { percent: f64 },
    SlowStorage { latency_ms: f64 },
    NetworkPartition,
}
```

---

## 8. Cloud vs Bare Metal

### 8.1 Cloud Considerations

**AWS Equivalents:**
```rust
enum AWSInstance {
    Basic => "c6i.2xlarge",      // 8 vCPU, 16 GB RAM
    Standard => "c6i.4xlarge",   // 16 vCPU, 32 GB RAM
    Performance => "c6i.8xlarge", // 32 vCPU, 64 GB RAM
    Archive => "m6i.8xlarge",    // 32 vCPU, 128 GB RAM
}

// Add NVMe instance storage or EBS io2 volumes
// Cost: $200-2000/month depending on tier
```

**Bare Metal Advantages:**
- No CPU steal time
- Predictable performance
- Lower long-term cost
- Full hardware control

**Cloud Advantages:**
- Fast deployment
- Easy scaling
- Geographic distribution
- Managed networking

**Recommendation:** Bare metal for validators, cloud for RPC/archive nodes.

---

## 9. Cost Analysis

### 9.1 Hardware Costs (Upfront)

| Tier            | Hardware | Power (3yr) | Total (3yr) |
| --------------- | -------- | ----------- | ----------- |
| **Basic**       | $1,800   | $790        | $2,590      |
| **Standard**    | $3,500   | $1,580      | $5,080      |
| **Performance** | $10,000  | $3,160      | $13,160     |
| **Archive**     | $20,000  | $3,950      | $23,950     |

*Assumes $0.12/kWh electricity*

### 9.2 Cloud Costs (Monthly)

| Tier            | Instance | Storage | Network | Total/mo | Total (3yr) |
| --------------- | -------- | ------- | ------- | -------- | ----------- |
| **Basic**       | $140     | $100    | $20     | $260     | $9,360      |
| **Standard**    | $280     | $200    | $50     | $530     | $19,080     |
| **Performance** | $560     | $400    | $100    | $1,060   | $38,160     |
| **Archive**     | $850     | $1200   | $200    | $2,250   | $81,000     |

**Break-even Analysis:**
- Basic: 10 months
- Standard: 10 months
- Performance: 12 months
- Archive: 13 months

**Conclusion:** Bare metal is more cost-effective for long-term operation.

---

## 10. Upgrade Path

### 10.1 Component Prioritization

**If budget-constrained, prioritize in this order:**

1. **Storage** (most critical)
   - NVMe Gen3 ‚Üí Gen4: +40% IOPS
   - Impact: Direct effect on TPS

2. **CPU** (high impact)
   - More cores ‚Üí higher parallelism
   - AVX-512 ‚Üí 2x PQ crypto speed

3. **RAM** (moderate impact)
   - More instances, better caching
   - ECC for reliability

4. **Network** (low impact until saturated)
   - 1 Gbps sufficient for most use cases
   - Upgrade to 10 Gbps only for RPC/archive

### 10.2 Future-Proofing

**Expected Hardware Evolution:**
```rust
// 2025 ‚Üí 2028 projections
struct HardwareTrends {
    cpu_cores: 32 => 64,           // More cores, same TDP
    cpu_freq: 4.5 => 5.5_GHz,      // Incremental gains
    avx_512_adoption: 60% => 95%,  // Near-universal by 2028
    
    ddr5_speed: 4800 => 8000_MT_s, // Faster memory
    ddr5_capacity: 128_GB => 512_GB, // Affordable high-capacity
    
    nvme_gen: 4 => 5,              // 2x sequential, +20% random
    nvme_cost_per_tb: $100 => $50, // Continued decline
    
    network: 1_Gbps => 10_Gbps,    // Symmetric fiber more common
}
```

**Recommendation:** Buy Performance tier specs today, they'll be Standard tier in 2028.

---

## 11. Security Considerations

### 11.1 Physical Security

**Validators (High Value Targets):**
- Secure physical location (datacenter or locked room)
- Uninterruptible Power Supply (UPS)
- Backup generator (optional for critical validators)
- Environmental monitoring (temperature, humidity)
- Fire suppression

### 11.2 Hardware Security

**Secure Boot:**
```bash
# Enable UEFI Secure Boot
# Prevents bootkits and rootkits
mokutil --sb-state
```

**TPM 2.0:**
```bash
# Use TPM for key sealing
# Validator keys encrypted at rest
apt install tpm2-tools
```

**IPMI/BMC Security:**
```bash
# Disable if not needed
# If needed: change default passwords, restrict network access
ipmitool lan set 1 access off
```

---

## 12. Conclusion

### 12.1 Recommended Starting Points

**New Validators:**
- Start with **Standard Tier**
- Proven hardware, manageable cost
- Room to grow to 10k TPS

**RPC Providers:**
- **Performance Tier** minimum
- Serve multiple clients
- SLA requirements

**Hobbyists/Developers:**
- **Basic Tier** acceptable
- Testnet participation
- Development/testing

### 12.2 Future Work

- [ ] GPU acceleration evaluation (ZK proofs)
- [ ] ARM server testing (AWS Graviton, Ampere)
- [ ] RISC-V feasibility study (long-term)
- [ ] Confidential computing (AMD SEV, Intel TDX)
- [ ] Specialized ASICs for PQ crypto (2027+)

---

**Document ID:** `ECLIPT-SC-HARDWARE-001`  
**Maintainer:** Ecliptica Protocol Engineering  
**License:** CC-BY-SA-4.0  
**Status:** Draft for Review