# Data Availability Sampling (DAS) Specification

## 1. Overview

Ecliptica's Data Availability Sampling (DAS) layer ensures that block data is available without requiring every node to download all data. This is critical for:
- **Light clients**: Can verify chain correctness by sampling a small subset of data
- **Scalability**: Validators don't need to store all historical data
- **Security**: Cryptographic guarantees that withheld data can be detected

**Design Principle**: Post-quantum secure (no KZG commitments), using only hash-based primitives.

---

## 2. Erasure Coding Parameters

### 2.1 Reed-Solomon Configuration

We use **systematic Reed-Solomon codes** for their efficiency and reconstruction properties.

| Parameter            | Value               | Rationale                                   |
| -------------------- | ------------------- | ------------------------------------------- |
| **k** (data chunks)  | 128                 | Original block data divided into 128 pieces |
| **n** (total chunks) | 256                 | 2× expansion (50% redundancy)               |
| **Expansion ratio**  | 2.0                 | Can reconstruct from any 128 of 256 chunks  |
| **Chunk size**       | 4 KB - 32 KB        | Depends on block size; typically 16 KB      |
| **Field**            | GF(2^8) or GF(2^16) | Byte-based or word-based Reed-Solomon       |

**Formula**: Given block data of size `B`, divide into `k = 128` chunks of size `s = B/128`. Encode to produce `n = 256` chunks total.

```rust
// Erasure coding parameters
const EC_DATA_CHUNKS: usize = 128;      // k
const EC_TOTAL_CHUNKS: usize = 256;     // n
const EC_PARITY_CHUNKS: usize = 128;    // n - k
const EC_EXPANSION_RATIO: f64 = 2.0;    // n/k

// Reconstruction threshold
const EC_RECONSTRUCTION_THRESHOLD: usize = 128;  // Need k chunks to reconstruct

// Field parameters (using GF(2^16) for efficiency)
const EC_FIELD_SIZE: usize = 65536;     // 2^16
```

### 2.2 Why These Parameters?

**k = 128 (50% data rate)**:
- Balance between reconstruction robustness and overhead
- Any 50% of chunks suffice for reconstruction
- Aligned with typical block sizes (128 chunks × 16 KB = 2 MB blocks)

**n = 256 (2× expansion)**:
- Provides high fault tolerance (up to 50% data loss)
- Reasonable bandwidth overhead
- Standard ratio used in distributed systems (Ethereum Danksharding uses similar)

**Chunk size 4-32 KB**:
- Large enough to amortize cryptographic proof overhead
- Small enough for efficient network transmission
- Aligned with typical network MTU and UDP packet sizes

---

## 3. Data Availability Block Structure

### 3.1 Block Commitment Format

Each block header contains a **Data Availability Commitment**:

```rust
struct DACommitment {
    // Block data metadata
    block_height: u64,
    block_hash: [u8; 32],
    shard_id: u8,
    
    // Erasure coding metadata
    data_size: u64,              // Original block data size in bytes
    chunk_size: u32,             // Size of each chunk in bytes
    k: u16,                      // Number of data chunks (128)
    n: u16,                      // Total number of chunks (256)
    
    // Merkle commitment to erasure-coded chunks
    chunks_merkle_root: [u8; 32],
    merkle_tree_depth: u8,       // log2(n) = 8 for n=256
    
    // Column commitments (for 2D erasure coding - future optimization)
    column_commitments: Option<Vec<[u8; 32]>>,
    
    // Timestamp
    timestamp: u64,
}
```

### 3.2 Chunk Merkle Tree Structure

The Merkle tree commits to all `n = 256` erasure-coded chunks.

```
                     Root (chunks_merkle_root)
                    /                         \
              H(0-127)                      H(128-255)
             /        \                    /          \
        H(0-63)    H(64-127)         H(128-191)   H(192-255)
        /    \      /    \             /    \       /    \
      ...   ...   ...   ...          ...   ...    ...   ...
     /  \  /  \  /  \  /  \         /  \  /  \   /  \  /  \
   C0  C1 C2 C3...                                      C255

Where: H() = SHAKE-256 hash
       Ci = Chunk i (data or parity)
```

**Leaf computation**:
```rust
fn compute_chunk_leaf(chunk: &[u8], chunk_index: u16) -> [u8; 32] {
    let mut hasher = Shake256::default();
    hasher.update(b"ECLIPT_DA_CHUNK_v1");
    hasher.update(&chunk_index.to_le_bytes());
    hasher.update(chunk);
    
    let mut output = [0u8; 32];
    hasher.finalize_xof_into(&mut output);
    output
}
```

**Internal node computation**:
```rust
fn compute_internal_node(left: &[u8; 32], right: &[u8; 32]) -> [u8; 32] {
    let mut hasher = Shake256::default();
    hasher.update(b"ECLIPT_DA_NODE_v1");
    hasher.update(left);
    hasher.update(right);
    
    let mut output = [0u8; 32];
    hasher.finalize_xof_into(&mut output);
    output
}
```

---

## 4. Sampling Strategy

### 4.1 Light Client Sampling Parameters

Light clients randomly sample chunks to verify data availability with high confidence.

| Parameter                 | Value          | Description                               |
| ------------------------- | -------------- | ----------------------------------------- |
| **Samples per block**     | 16             | Number of chunks sampled per block        |
| **Confidence level**      | 99.99%         | Probability of detecting unavailable data |
| **Sampling distribution** | Uniform random | Each chunk has equal probability          |
| **Sampling period**       | Per epoch (1s) | Sample once per finalized epoch           |

**Security analysis**: 
- If an attacker withholds >50% of chunks (>128 chunks), they cannot reconstruct the block
- With 16 random samples, probability of detecting withholding:

```
P(detect) = 1 - (1 - p_withheld)^samples
```

For `p_withheld = 0.51` (just over threshold) and `samples = 16`:
```
P(detect) = 1 - (0.49)^16 ≈ 1 - 0.000019 ≈ 99.998%
```

For `p_withheld = 0.75` (attacker withholds 75%):
```
P(detect) = 1 - (0.25)^16 ≈ 1 - 2.3×10^-10 ≈ 99.9999999%
```

### 4.2 Sampling Algorithm

```rust
struct DASRequest {
    block_height: u64,
    block_hash: [u8; 32],
    chunk_indices: Vec<u16>,      // Which chunks to retrieve
    requester_id: PeerId,
}

struct DASResponse {
    block_height: u64,
    block_hash: [u8; 32],
    chunks: Vec<ChunkWithProof>,
}

struct ChunkWithProof {
    chunk_index: u16,
    chunk_data: Vec<u8>,
    merkle_proof: Vec<[u8; 32]>,  // Path from leaf to root
}

// Light client sampling
fn perform_das_sampling(
    block_header: &BlockHeader,
    num_samples: usize,
) -> Result<bool> {
    // 1. Generate random sample indices deterministically
    let sample_indices = generate_sample_indices(
        &block_header.block_hash,
        &block_header.chunks_merkle_root,
        num_samples,
        EC_TOTAL_CHUNKS,
    );
    
    // 2. Request chunks from network
    let request = DASRequest {
        block_height: block_header.height,
        block_hash: block_header.block_hash,
        chunk_indices: sample_indices,
        requester_id: local_peer_id(),
    };
    
    let responses = broadcast_das_request(request).await?;
    
    // 3. Verify each chunk
    for chunk_with_proof in responses.chunks {
        let valid = verify_chunk_inclusion(
            &chunk_with_proof,
            &block_header.chunks_merkle_root,
        )?;
        
        if !valid {
            return Ok(false);  // Data availability failure detected
        }
    }
    
    Ok(true)  // All samples verified successfully
}

fn generate_sample_indices(
    block_hash: &[u8; 32],
    merkle_root: &[u8; 32],
    num_samples: usize,
    total_chunks: usize,
) -> Vec<u16> {
    // Deterministic sampling using block hash as seed
    let seed = {
        let mut hasher = Shake256::default();
        hasher.update(b"ECLIPT_DAS_SAMPLING_v1");
        hasher.update(block_hash);
        hasher.update(merkle_root);
        
        let mut seed = [0u8; 32];
        hasher.finalize_xof_into(&mut seed);
        seed
    };
    
    let mut rng = ChaCha20Rng::from_seed(seed);
    
    // Generate unique random indices
    let mut indices = HashSet::new();
    while indices.len() < num_samples {
        let index = rng.gen_range(0..total_chunks as u16);
        indices.insert(index);
    }
    
    indices.into_iter().collect()
}
```

### 4.3 Validator Sampling (Full Nodes)

Validators have stricter requirements:

| Node Type           | Sampling Requirement         | Purpose                              |
| ------------------- | ---------------------------- | ------------------------------------ |
| **Full Validator**  | Download ALL chunks (n=256)  | Execute blocks, produce zk-STARKs    |
| **Archival Node**   | Store ALL chunks permanently | Historical queries, block explorers  |
| **Light Validator** | Sample 16 chunks per block   | Verify availability, fraud detection |
| **DA Node**         | Store 128+ chunks per block  | Serve data to light clients          |

---

## 5. Reconstruction Procedure

### 5.1 Reconstruction Algorithm

When a node needs to reconstruct the original block data from available chunks:

```rust
fn reconstruct_block_data(
    available_chunks: Vec<(u16, Vec<u8>)>,  // (index, data) pairs
    ec_params: &ECParams,
) -> Result<Vec<u8>> {
    // 1. Check if we have enough chunks
    if available_chunks.len() < ec_params.k {
        return Err(Error::InsufficientChunks {
            available: available_chunks.len(),
            required: ec_params.k,
        });
    }
    
    // 2. Sort chunks by index
    let mut chunks = available_chunks;
    chunks.sort_by_key(|(idx, _)| *idx);
    
    // 3. Select first k chunks for reconstruction
    let reconstruction_set: Vec<_> = chunks.iter()
        .take(ec_params.k)
        .collect();
    
    // 4. Use Reed-Solomon decoder
    let decoder = ReedSolomonDecoder::new(ec_params.k, ec_params.n)?;
    
    let original_data = decoder.reconstruct(reconstruction_set)?;
    
    // 5. Verify reconstruction
    let reconstructed_hash = compute_block_hash(&original_data);
    if reconstructed_hash != expected_block_hash {
        return Err(Error::ReconstructionVerificationFailed);
    }
    
    Ok(original_data)
}

// Reed-Solomon reconstruction (simplified)
impl ReedSolomonDecoder {
    fn reconstruct(
        &self,
        chunks: Vec<&(u16, Vec<u8>)>,
    ) -> Result<Vec<u8>> {
        // Extract indices and data
        let indices: Vec<usize> = chunks.iter()
            .map(|(idx, _)| *idx as usize)
            .collect();
        
        let data: Vec<Vec<u8>> = chunks.iter()
            .map(|(_, data)| (*data).clone())
            .collect();
        
        // Perform polynomial interpolation in GF(2^16)
        let original_chunks = self.lagrange_interpolate(&indices, &data)?;
        
        // Concatenate original k chunks
        let mut result = Vec::new();
        for chunk in original_chunks {
            result.extend_from_slice(&chunk);
        }
        
        Ok(result)
    }
}
```

### 5.2 Reconstruction Scenarios

**Scenario 1: Full node loses some chunks**
- Node has 200 out of 256 chunks stored locally
- Reconstruction succeeds (>128 chunks available)
- No network requests needed

**Scenario 2: Light client needs block data**
- Light client has sampled 16 chunks
- Requests additional chunks from DA nodes (needs 128 total)
- Reconstructs full block

**Scenario 3: Network partition**
- Different nodes have different subsets of chunks
- Nodes gossip available chunks
- Each node reconstructs locally once they have ≥128 chunks

---

## 6. Fisherman Protocol (Fraud Proof Mechanism)

### 6.1 Fraud Proof Types

| Fraud Type                 | Detection Method                   | Penalty           |
| -------------------------- | ---------------------------------- | ----------------- |
| **Data Withholding**       | Failure to serve sampled chunks    | Slash 1-5% stake  |
| **Invalid Erasure Coding** | Chunks don't reconstruct correctly | Slash 5-10% stake |
| **Invalid Merkle Proof**   | Proof doesn't match commitment     | Slash 5-10% stake |
| **Chunk Corruption**       | Chunk hash doesn't match leaf      | Slash 1-5% stake  |

### 6.2 Fisherman Workflow

```rust
struct FraudProof {
    proof_type: FraudProofType,
    block_height: u64,
    block_hash: [u8; 32],
    accused_peer: PeerId,
    
    evidence: FraudEvidence,
    
    // Fisherman identity
    fisherman: Address,
    signature: DilithiumSignature,
}

enum FraudProofType {
    DataWithholding,
    InvalidErasureCoding,
    InvalidMerkleProof,
    ChunkCorruption,
}

enum FraudEvidence {
    // Data withholding: Proof that node refused to serve chunk
    WithholdingEvidence {
        request_timestamp: u64,
        request_signature: DilithiumSignature,
        timeout_proof: Vec<u8>,  // Network logs
    },
    
    // Invalid erasure coding: Chunks that don't reconstruct
    InvalidCodingEvidence {
        chunk_indices: Vec<u16>,
        chunks: Vec<Vec<u8>>,
        merkle_proofs: Vec<Vec<[u8; 32]>>,
        reconstruction_error: String,
    },
    
    // Invalid Merkle proof
    InvalidProofEvidence {
        chunk_index: u16,
        chunk_data: Vec<u8>,
        merkle_proof: Vec<[u8; 32]>,
        claimed_root: [u8; 32],
    },
    
    // Chunk corruption
    CorruptionEvidence {
        chunk_index: u16,
        chunk_data: Vec<u8>,
        expected_hash: [u8; 32],
        actual_hash: [u8; 32],
    },
}
```

### 6.3 Fraud Proof Verification

```rust
fn verify_fraud_proof(proof: &FraudProof) -> Result<bool> {
    match &proof.evidence {
        FraudEvidence::WithholdingEvidence { 
            request_timestamp,
            request_signature,
            timeout_proof,
        } => {
            // 1. Verify request was legitimate
            verify_das_request_signature(request_signature)?;
            
            // 2. Verify sufficient time elapsed (>5 seconds)
            let current = current_timestamp();
            if current - request_timestamp < 5000 {
                return Ok(false);  // Not enough time to respond
            }
            
            // 3. Verify no response was received
            verify_timeout_proof(timeout_proof)?;
            
            Ok(true)
        }
        
        FraudEvidence::InvalidCodingEvidence {
            chunk_indices,
            chunks,
            merkle_proofs,
            reconstruction_error,
        } => {
            // 1. Verify all chunks are included in commitment
            for (i, proof) in merkle_proofs.iter().enumerate() {
                let valid = verify_chunk_inclusion_proof(
                    chunk_indices[i],
                    &chunks[i],
                    proof,
                    &proof.block_hash,
                )?;
                
                if !valid {
                    return Ok(false);  // Fraudulent fraud proof!
                }
            }
            
            // 2. Attempt reconstruction
            let result = reconstruct_block_data(
                chunk_indices.iter()
                    .zip(chunks.iter())
                    .map(|(idx, data)| (*idx, data.clone()))
                    .collect(),
                &ECParams::default(),
            );
            
            // 3. Reconstruction should fail or mismatch
            match result {
                Ok(data) => {
                    let hash = compute_block_hash(&data);
                    Ok(hash != proof.block_hash)  // Mismatch = fraud
                }
                Err(_) => Ok(true),  // Reconstruction failure = fraud
            }
        }
        
        FraudEvidence::InvalidProofEvidence { 
            chunk_index,
            chunk_data,
            merkle_proof,
            claimed_root,
        } => {
            // Verify proof is invalid
            let computed_root = compute_merkle_root_from_proof(
                *chunk_index,
                chunk_data,
                merkle_proof,
            )?;
            
            Ok(computed_root != *claimed_root)
        }
        
        FraudEvidence::CorruptionEvidence {
            chunk_index,
            chunk_data,
            expected_hash,
            actual_hash,
        } => {
            // Verify corruption
            let computed = compute_chunk_leaf(chunk_data, *chunk_index);
            
            Ok(computed == *actual_hash && *actual_hash != *expected_hash)
        }
    }
}
```

### 6.4 Fisherman Incentives

```rust
struct FishermanReward {
    // Fixed reward for valid fraud proof
    base_reward: u64,              // 100 ECLIPT
    
    // Portion of slashed stake
    slash_share: f64,              // 10% of slashed amount
    
    // Priority for future DA requests
    priority_boost: u32,           // +10 priority score
}

const BASE_FISHERMAN_REWARD: u64 = 100 * ECLIPT_PER_COIN;
const SLASH_SHARE_PERCENTAGE: f64 = 0.10;

fn process_fraud_proof_reward(
    proof: &FraudProof,
    slash_amount: u64,
) -> Result<()> {
    // 1. Base reward
    mint_to_account(proof.fisherman, BASE_FISHERMAN_REWARD)?;
    
    // 2. Share of slash
    let slash_reward = (slash_amount as f64 * SLASH_SHARE_PERCENTAGE) as u64;
    transfer_from_slash_pool(proof.fisherman, slash_reward)?;
    
    // 3. Update fisherman reputation
    increase_fisherman_reputation(proof.fisherman, 10)?;
    
    Ok(())
}
```

---

## 7. Storage Commitments Format

### 7.1 On-Chain DA Commitment

Stored in block header:

```rust
struct BlockHeader {
    // ... other fields ...
    
    // Data availability commitment
    da_commitment: DACommitment,
}

impl DACommitment {
    // Canonical serialization for hashing
    fn serialize_canonical(&self) -> Vec<u8> {
        let mut buf = Vec::new();
        
        buf.extend_from_slice(&self.block_height.to_le_bytes());
        buf.extend_from_slice(&self.block_hash);
        buf.push(self.shard_id);
        buf.extend_from_slice(&self.data_size.to_le_bytes());
        buf.extend_from_slice(&self.chunk_size.to_le_bytes());
        buf.extend_from_slice(&self.k.to_le_bytes());
        buf.extend_from_slice(&self.n.to_le_bytes());
        buf.extend_from_slice(&self.chunks_merkle_root);
        buf.push(self.merkle_tree_depth);
        buf.extend_from_slice(&self.timestamp.to_le_bytes());
        
        buf
    }
    
    // Compute commitment hash (included in block header)
    fn commitment_hash(&self) -> [u8; 32] {
        let mut hasher = Shake256::default();
        hasher.update(b"ECLIPT_DA_COMMITMENT_v1");
        hasher.update(&self.serialize_canonical());
        
        let mut output = [0u8; 32];
        hasher.finalize_xof_into(&mut output);
        output
    }
}
```

### 7.2 Off-Chain Storage (DA Nodes)

DA nodes store:

1. **Full erasure-coded chunks**: All 256 chunks for recent epochs
2. **Merkle tree**: Complete tree for proof generation
3. **Metadata index**: Fast lookup by (block_height, chunk_index)

```rust
struct DANodeStorage {
    // Chunk storage (key: block_height:chunk_index)
    chunks: RocksDB,
    
    // Merkle tree storage
    merkle_trees: RocksDB,
    
    // Metadata index
    metadata: RocksDB,
}

// Key format: "chunk:{block_height}:{chunk_index}"
fn chunk_key(block_height: u64, chunk_index: u16) -> String {
    format!("chunk:{}:{:03}", block_height, chunk_index)
}

// Key format: "merkle:{block_height}:{node_index}"
fn merkle_node_key(block_height: u64, node_index: u32) -> String {
    format!("merkle:{}:{}", block_height, node_index)
}

// Store chunk with proof
fn store_chunk(
    db: &DANodeStorage,
    block_height: u64,
    chunk_index: u16,
    chunk_data: &[u8],
    merkle_proof: &[[u8; 32]],
) -> Result<()> {
    let key = chunk_key(block_height, chunk_index);
    
    let value = ChunkStorageValue {
        chunk_data: chunk_data.to_vec(),
        merkle_proof: merkle_proof.to_vec(),
        timestamp: current_timestamp(),
    };
    
    db.chunks.put(key, serialize(&value))?;
    
    Ok(())
}
```

### 7.3 Pruning Strategy

DA nodes prune old data according to retention policies:

| Data Type             | Retention    | Rationale                     |
| --------------------- | ------------ | ----------------------------- |
| **Recent chunks**     | 30 days      | Active sampling, fraud proofs |
| **Historical chunks** | 1 year       | Reconstruction, syncing       |
| **Archival chunks**   | Forever      | Archive nodes only            |
| **Merkle trees**      | Match chunks | Needed for proof generation   |

```rust
const RECENT_RETENTION_EPOCHS: u64 = 30 * 24 * 3600;  // 30 days
const HISTORICAL_RETENTION_EPOCHS: u64 = 365 * 24 * 3600;  // 1 year

fn prune_old_chunks(
    db: &DANodeStorage,
    current_epoch: u64,
    retention_policy: RetentionPolicy,
) -> Result<()> {
    let cutoff_epoch = match retention_policy {
        RetentionPolicy::Recent => {
            current_epoch.saturating_sub(RECENT_RETENTION_EPOCHS)
        }
        RetentionPolicy::Historical => {
            current_epoch.saturating_sub(HISTORICAL_RETENTION_EPOCHS)
        }
        RetentionPolicy::Archival => return Ok(()),  // Never prune
    };
    
    // Iterate and delete old chunks
    let prefix = "chunk:".to_string();
    let mut iter = db.chunks.iterator_from(&prefix);
    
    while let Some((key, _)) = iter.next() {
        let parts: Vec<&str> = key.split(':').collect();
        if parts.len() >= 2 {
            if let Ok(epoch) = parts[1].parse::<u64>() {
                if epoch < cutoff_epoch {
                    db.chunks.delete(key)?;
                    
                    // Also delete corresponding Merkle nodes
                    delete_merkle_tree(db, epoch)?;
                }
            }
        }
    }
    
    Ok(())
}
```

---

## 8. Network Protocol

### 8.1 DAS Request/Response Protocol

Using libp2p request-response protocol:

```rust
// Protocol: /ecliptica/das/1.0.0
struct DASProtocol;

impl RequestResponseCodec for DASProtocol {
    type Protocol = String;
    type Request = DASRequest;
    type Response = DASResponse;
    
    async fn read_request<T>(
        &mut self,
        _: &Self::Protocol,
        io: &mut T,
    ) -> Result<Self::Request>
    where
        T: AsyncRead + Unpin,
    {
        // Read length-prefixed request
        let mut len_buf = [0u8; 4];
        io.read_exact(&mut len_buf).await?;
        let len = u32::from_le_bytes(len_buf) as usize;
        
        let mut buf = vec![0u8; len];
        io.read_exact(&mut buf).await?;
        
        // Deserialize using bincode
        Ok(bincode::deserialize(&buf)?)
    }
    
    async fn read_response<T>(
        &mut self,
        _: &Self::Protocol,
        io: &mut T,
    ) -> Result<Self::Response>
    where
        T: AsyncRead + Unpin,
    {
        // Similar to read_request
        let mut len_buf = [0u8; 4];
        io.read_exact(&mut len_buf).await?;
        let len = u32::from_le_bytes(len_buf) as usize;
        
        let mut buf = vec![0u8; len];
        io.read_exact(&mut buf).await?;
        
        Ok(bincode::deserialize(&buf)?)
    }
    
    async fn write_request<T>(
        &mut self,
        _: &Self::Protocol,
        io: &mut T,
        req: Self::Request,
    ) -> Result<()>
    where
        T: AsyncWrite + Unpin,
    {
        let data = bincode::serialize(&req)?;
        let len = (data.len() as u32).to_le_bytes();
        
        io.write_all(&len).await?;
        io.write_all(&data).await?;
        
        Ok(())
    }
    
    async fn write_response<T>(
        &mut self,
        _: &Self::Protocol,
        io: &mut T,
        res: Self::Response,
    ) -> Result<()>
    where
        T: AsyncWrite + Unpin,
    {
        let data = bincode::serialize(&res)?;
        let len = (data.len() as u32).to_le_bytes();
        
        io.write_all(&len).await?;
        io.write_all(&data).await?;
        
        Ok(())
    }
}
```

### 8.2 Gossip Topics

DA chunks are also gossiped for redundancy:

| Topic                            | Purpose                  | Message Rate         |
| -------------------------------- | ------------------------ | -------------------- |
| `/ecliptica/das/chunks/v1`       | Broadcast chunks         | Per-chunk as encoded |
| `/ecliptica/das/commitments/v1`  | Broadcast DA commitments | Per-block            |
| `/ecliptica/das/fraud-proofs/v1` | Broadcast fraud proofs   | Rare (on fraud)      |

---

## 9. Performance Analysis

### 9.1 Bandwidth Requirements

**Full Validator** (downloads all chunks):
- Block size: 2 MB
- Erasure-coded size: 4 MB (2× expansion)
- Merkle proofs: ~8 KB per chunk × 256 = 2 MB
- **Total**: ~6 MB per block
- At 2 blocks/sec: **12 MB/s = 96 Mbps**

**Light Client** (samples 16 chunks):
- Chunk size: 16 KB × 16 = 256 KB
- Merkle proofs: 8 KB × 16 = 128 KB
- **Total**: ~384 KB per block
- At 1 epoch/sec: **384 KB/s = 3 Mbps**

**DA Node** (stores and serves chunks):
- Storage: 4 MB × 86400 blocks/day = 345 GB/day
- With 30-day retention: **10.4 TB**
- With 1-year retention: **126 TB**

### 9.2 Computational Cost

**Erasure Encoding** (validator):
- Reed-Solomon encoding: ~50-100 MB/s
- For 2 MB block: **20-40 ms**

**Merkle Tree Construction**:
- 256 leaves, depth 8
- SHAKE-256 hashing: ~500 MB/s
- For 256 × 32-byte hashes: **<1 ms**

**Chunk Verification** (light client):
- Merkle proof verification: 8 hashes
- SHAKE-256: ~500 MB/s
- Per chunk: **<0.1 ms**
- For 16 chunks: **<2 ms**

### 9.3 Security Parameters

| Parameter                    | Value  | Security Level              |
| ---------------------------- | ------ | --------------------------- |
| **False negative rate**      | <0.01% | 99.99% confidence           |
| **False positive rate**      | ~0%    | Cryptographic hash security |
| **Reconstruction threshold** | 50%    | Byzantine fault tolerance   |
| **Maximum withholding**      | 49%    | Cannot block reconstruction |

---

## 10. Implementation Checklist

- [ ] Implement Reed-Solomon encoder/decoder (use `reed-solomon-erasure` crate)
- [ ] Build Merkle tree construction and proof generation
- [ ] Implement DAS sampling algorithm
- [ ] Create fisherman fraud proof protocol
- [ ] Build DA node storage backend (RocksDB)
- [ ] Implement network protocols (request-response + gossip)
- [ ] Add pruning and retention policies
- [ ] Create light client sampling SDK
- [ ] Write comprehensive tests (unit, integration, adversarial)
- [ ] Add metrics and monitoring (Prometheus)
- [ ] Document operational runbooks

---

## 11. Future Optimizations

### 11.1 2D Erasure Coding

Extend to 2D Reed-Solomon (like Ethereum Danksharding):
- Arrange data in k×k matrix
- Encode rows and columns separately
- More efficient sampling (row/column commitments)

### 11.2 Proof Aggregation

Use zk-STARKs to aggregate Merkle proofs:
- Single proof for multiple chunks
- Reduces verification cost for light clients

### 11.3 Adaptive Sampling

Adjust sampling parameters based on network conditions:
- Increase samples during attacks
- Decrease samples when network is healthy
- Use reputation to prioritize reliable DA nodes